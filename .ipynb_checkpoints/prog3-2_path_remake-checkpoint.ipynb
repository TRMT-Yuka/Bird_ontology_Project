{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pickleから辞書を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # （tsv形式の辞書からpickleを取得）\n",
    "\n",
    "# with open('data_extract/ontology.tsv', mode='r', newline='', encoding='utf-8') as f:\n",
    "#     tsv_reader = csv.reader(f, delimiter='\\t')\n",
    "#     data = [row for row in tsv_reader if row != []]\n",
    "\n",
    "# new_data = []\n",
    "# key = data[0]\n",
    "\n",
    "# for d in data[1:]:\n",
    "#     data_d = {}\n",
    "#     for i in range(len(key)):\n",
    "#         data_d[key[i]] = d[i]\n",
    "#     new_data.append(data_d)\n",
    "\n",
    "# import pickle\n",
    "# with open(\"data_extract/ontology.pickle\", 'wb') as f:\n",
    "#     pickle.dump(new_data,f)\n",
    "\n",
    "import pickle\n",
    "with open(\"data_extract/ontology.pickle\", 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepL API準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data_secret/DeepL_api.txt', 'r', encoding='UTF-8') as f:\n",
    "#     k = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def ENtoJA(text):\n",
    "    # URLクエリに仕込むパラメータの辞書を作っておく\n",
    "    params = {\"auth_key\": k,\n",
    "                \"text\": text,\n",
    "                \"source_lang\": 'EN', # 入力テキストの言語を英語に設定\n",
    "                \"target_lang\": 'JA'  # 出力テキストの言語を日本語に設定（JPではなくJAなので注意）\n",
    "             }\n",
    "\n",
    "    # パラメータと一緒にPOSTする\n",
    "    request = requests.post(\"https://api-free.deepl.com/v2/translate\", data=params) # free用のURL、有料版はURLが異なります\n",
    "    result = request.json()\n",
    "    return result[\"translations\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    if \"trans_name\" in d:\n",
    "        pass\n",
    "    else:\n",
    "        d[\"trans_name\"] = ENtoJA(d[\"en_name\"])\n",
    "        print('\\r%s                                            ' %(d[\"trans_name\"]), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"DeepL_11057in50640.bin\", 'wb') as f:\n",
    "    pickle.dump(data,f)\n",
    "    \n",
    "# 翻訳結果\n",
    "# ・50640項目中11057の地点でAPIの利用制限到達，翻訳精度微妙\n",
    "# ・英語を直に日本語読みとかのほうがまだいいかも\n",
    "# ・作成したデータは上記のバイナリファイルに保存してtrushに移動済み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"trush/DeepL_11057in50640.bin\", 'rb') as f:\n",
    "    DeepL_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワーク作成　（&親子tsv作成)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in c:\\users\\teramoto\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\teramoto\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_parent_data = []\n",
    "for d in data:\n",
    "    child_parent_data.append({\"id\":d[\"id\"],\"parent_taxon\":d[\"parent_taxon\"]})\n",
    "\n",
    "import csv\n",
    "with open('data_extract/child_parent.tsv','w',encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = [\"id\",\"parent_taxon\"],delimiter = \"\\t\")\n",
    "    writer.writeheader()\n",
    "    writer.writerows(child_parent_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.DiGraph() #無向グラフを作成 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in child_parent_data:\n",
    "    G.add_edge(d[\"parent_taxon\"],d[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "nx.draw(G, with_labels = True)\n",
    "plt.show()\n",
    "\n",
    "#描画試みの結果\n",
    "# ・データが大きすぎるため私の個人PCでは不可能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パスデータの抜け確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 言語によるパスおよびそのランク表現を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {}\n",
    "for d in data:\n",
    "    dict_data[d[\"id\"]]=d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    if \"path_name\" in d:\n",
    "        pass\n",
    "    else:\n",
    "        path_list = d[\"path\"].replace(\"'\",\"\")\n",
    "        d[\"path_name\"] = conversion.do(d[\"ja_name\"])\n",
    "        print('\\r%s                                            ' %(d[\"romaji_name\"]), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    path_list = d[\"path\"].replace(\"'/\",\"\").replace(\"'\",\"\").split(\"/\")\n",
    "    path_list\n",
    "\n",
    "    ja_path_name = []\n",
    "    en_path_name = []\n",
    "    path_taxon_rank_name = []\n",
    "\n",
    "    for path in path_list:\n",
    "        if path in dict_data:\n",
    "            if dict_data[path][\"ja_name\"] != \"\":\n",
    "                ja_path_name.append(dict_data[path][\"ja_name\"])\n",
    "                en_path_name.append(dict_data[path][\"en_name\"])\n",
    "            else:\n",
    "                ja_path_name.append(dict_data[path][\"en_name\"])\n",
    "                en_path_name.append(dict_data[path][\"en_name\"])\n",
    "\n",
    "            if dict_data[path][\"taxon_rank_name\"] != \"\":\n",
    "                path_taxon_rank_name.append(dict_data[path][\"taxon_rank_name\"])\n",
    "        else:\n",
    "            ja_path_name.append(path)\n",
    "            path_taxon_rank_name.append(\"Unknown\")\n",
    "\n",
    "    d[\"path\"] = \"/\".join(path_list)\n",
    "    d[\"ja_path_name\"] = \"/\".join(ja_path_name)\n",
    "    d[\"en_path_name\"] = \"/\".join(en_path_name)\n",
    "    d[\"path_taxon_rank_name\"] = \"/\".join(path_taxon_rank_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedname_list = ['id','en_name','ja_name','en_aliases','ja_aliases','img_urls','taxon_name','taxon_rank','taxon_rank_name','taxon_rank_ja_name','parent_taxon','parent_taxon_name','parent_taxon_ja_name',\"path\",\"ja_path_name\",\"en_path_name\",\"path_taxon_rank_name\"]\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('data_extract/ontology_path.tsv','w',encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = filedname_list,delimiter = \"\\t\")\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "    \n",
    "with open(\"data_extract/ontology_path.pickle\", 'wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### パスのランク種をtxtに集計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_set = set()\n",
    "for d in data:\n",
    "    path_set.add(d[\"path_taxon_rank_name\"])\n",
    "    \n",
    "path_list = list(path_set)\n",
    "path_list.sort()\n",
    "\n",
    "with open('data_extract/path_category.txt', 'w') as f:\n",
    "    for row in path_list:\n",
    "        f.write(row)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "#完全な場合のパス\n",
    "complete_path = [\"subclass\",\"infraclass\",\"superorder\",\"order\",\"suborder\",\"infraorder\",\"superfamily\",\"family\",\"subfamily\",\"tribe\",\"genus\",\"subgenus\",\"species\",\"subspecies\",\"form\"]\n",
    "\n",
    "# 完全版パスを人手で復元\n",
    "# すべて人力で修正していくと持たないので，泣き声データにあるもののみ人手で修正することに"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模範"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
